{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06798535-4f12-4756-be8d-cff8578364d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\sklearn\\__init__.py:70\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build, _distributor_init  \u001b[38;5;66;03m# noqa: E402 F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     73\u001b[39m _submodules = [\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_pandas_na, is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, resample, shuffle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\scipy\\sparse\\__init__.py:305\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_importlib\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\ML\\venv\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sparsetools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[32m     12\u001b[39m                            get_csr_submatrix, csr_sample_values)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compressed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "train = pd.read_csv(r\"D:\\code\\ML\\Datasets\\titanic_train.csv\")\n",
    "test = pd.read_csv(r\"D:\\code\\ML\\Datasets\\titanic_test.csv\")\n",
    "test_id = test[\"PassengerId\"]\n",
    "#training data general preprocessing\n",
    "titanic_label = train[\"Survived\"] #spliting target label\n",
    "titanic = train.drop(\"Survived\", axis=1) #removing taregt\n",
    "titanic[\"Deck\"] = titanic[\"Cabin\"].str[0] #extracting deck(C) letter from cabin(C72)\n",
    "#replacing null values in deck column according to Pcalss of the passenger(Pclass{1: (A,B,C), 2:(D,E), 3:(F,G,T)})\n",
    "titanic.loc[titanic[\"Deck\"].isnull() & (titanic[\"Pclass\"]==1), \"Deck\"] = \"B\" \n",
    "titanic.loc[titanic[\"Deck\"].isnull() & (titanic[\"Pclass\"]==2), \"Deck\"] = \"D\"\n",
    "titanic.loc[titanic[\"Deck\"].isnull() & (titanic[\"Pclass\"]==3), \"Deck\"] = \"F\"\n",
    "#replacing null values in emarbarked to most frequent value\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(titanic[\"Embarked\"].mode()[0])\n",
    "\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"] + 1\n",
    "titanic[\"IsAlone\"] = (titanic[\"FamilySize\"] == 1).astype(int)\n",
    "titanic[\"Title\"] = titanic[\"Name\"].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Mlle','Ms'],'Miss')\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace('Mme','Mrs')\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Capt','Col','Major','Dr','Rev'], 'Officer')\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Don','Dona','Lady','Countess','Jonkheer','Sir'], 'Noble')\n",
    "\n",
    "#dropping attribute i think not useful anymore\n",
    "drop_attribute = [\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"]\n",
    "titanic = titanic.drop(drop_attribute, axis=1)\n",
    "#category and numerical attribute\n",
    "#cat_attribute = [\"Sex\", \"Embarked\", \"Deck\"]\n",
    "#num_attribute = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "cat_attributes = titanic.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_attributes = titanic.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (num_pipeline, num_attributes),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), cat_attributes),\n",
    ")\n",
    "titanic_transformed = preprocessor.fit_transform(titanic)\n",
    "columns = preprocessor.get_feature_names_out()\n",
    "titanic_transformed_df = pd.DataFrame(titanic_transformed, columns=columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_transformed, titanic_label, random_state=42)\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "svc = SVC(random_state=42)\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[\n",
    " ('lr', lr),\n",
    " ('rf', rf),\n",
    " ('svc', svc)\n",
    " ]\n",
    " )\n",
    "param_distributions = {\n",
    "    #logistic regression\n",
    "    'lr__C': uniform(0.01,10),\n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__solver': ['liblinear'],\n",
    "\n",
    "    # Random Forest\n",
    "    'rf__n_estimators': randint(50,175),\n",
    "    'rf__max_depth': [None, 5, 10, 15],\n",
    "    'rf__max_features': ['sqrt', 'log2'],\n",
    "    'rf__min_samples_split': randint(2, 10),\n",
    "\n",
    "    # SVC\n",
    "    'svc__C': uniform(0.1,6),\n",
    "    'svc__kernel': ['rbf'],\n",
    "    'svc__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=voting_clf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=45,\n",
    "    cv=4,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-val score:\", random_search.best_score_)\n",
    "print(\"Test score:\", random_search.score(X_test, y_test))\n",
    "\n",
    "#hyperparameter tuning accoriding to rs\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "lr_best = LogisticRegression(\n",
    "    C=best_params['lr__C'],\n",
    "    penalty=best_params['lr__penalty'],\n",
    "    solver=best_params['lr__solver'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=best_params['rf__n_estimators'],\n",
    "    max_depth=best_params['rf__max_depth'],\n",
    "    max_features=best_params['rf__max_features'],\n",
    "    min_samples_split=best_params['rf__min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svc_best = SVC(\n",
    "    C=best_params['svc__C'],\n",
    "    kernel=best_params['svc__kernel'],\n",
    "    gamma=best_params['svc__gamma'],\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Voting classifier (soft voting usually performs better)\n",
    "voting_clf_2 = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_best),\n",
    "        ('rf', rf_best),\n",
    "        ('svc', svc_best)\n",
    "    ],\n",
    "    voting='soft', \n",
    "    n_jobs=-1\n",
    ")\n",
    "voting_clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Train Accuracy:\", voting_clf_2.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", voting_clf_2.score(X_test, y_test))\n",
    "\n",
    "y_pred_tuned = voting_clf_2.predict(X_test)\n",
    "print(\"Accuracy:\", voting_clf_2.score(X_test, y_test))\n",
    "\n",
    "cross_val_score(voting_clf_2, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "for name, clf in voting_clf_2.named_estimators_.items():     \n",
    "    print(name, \"=\", clf.score(X_test, y_test))\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Accuracy:\", voting_clf.score(X_test, y_test))\n",
    "\n",
    "cross_val_score(voting_clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "for name, clf in voting_clf.named_estimators_.items():     \n",
    "    print(name, \"=\", clf.score(X_test, y_test))\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    " estimators=[\n",
    " ('lr',lr_best),\n",
    " ('rf', rf_best),\n",
    " ('svc', svc_best)\n",
    " ],\n",
    " final_estimator=RandomForestClassifier(random_state=43),\n",
    " cv=5  # number of cross-validation folds\n",
    " )\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "print(\"Train Accuracy:\", stacking_clf.score(X_train, y_train))\n",
    "cross_val_score(stacking_clf, X_train, y_train, cv=5, scoring=\"accuracy\")  \n",
    "\n",
    "#test data preprocessing\n",
    "test[\"Deck\"] = test[\"Cabin\"].str[0]\n",
    "test.loc[test[\"Deck\"].isnull() & (test[\"Pclass\"]==1), \"Deck\"] = \"B\"\n",
    "test.loc[test[\"Deck\"].isnull() & (test[\"Pclass\"]==2), \"Deck\"] = \"D\"\n",
    "test.loc[test[\"Deck\"].isnull() & (test[\"Pclass\"]==3), \"Deck\"] = \"F\"\n",
    "test[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")\n",
    "test[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n",
    "test[\"IsAlone\"] = (test[\"FamilySize\"] == 1).astype(int)\n",
    "test[\"Title\"] = test[\"Name\"].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "test[\"Title\"] = test[\"Title\"].replace(['Mlle','Ms'],'Miss')\n",
    "test[\"Title\"] = test[\"Title\"].replace('Mme','Mrs')\n",
    "test[\"Title\"] = test[\"Title\"].replace(['Capt','Col','Major','Dr','Rev'], 'Officer')\n",
    "test[\"Title\"] = test[\"Title\"].replace(['Don','Dona','Lady','Countess','Jonkheer','Sir'], 'Noble')\n",
    "test = test.drop(drop_attribute, axis=1)\n",
    "test_transformed = preprocessor.transform(test)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "test_transformed_df = pd.DataFrame(test_transformed, columns=feature_names)\n",
    "\n",
    "test_transformed_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
